{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddpg import ReplayBuffer, DDPGAgent, EpsilonGreedy\n",
    "from fsae.envs import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    env.close()\n",
    "except:\n",
    "    pass \n",
    "env = RandomTrackEnv(render_mode=None, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = np.random.normal(0,0.5, 1000)\n",
    "plt.hist(samples, bins=20, edgecolor='black')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Average Episode Reward: 2.795785486871863 Replay buffer size: 14\n",
      "saved weights\n",
      "10 Average Episode Reward: 3.368977213898318 Replay buffer size: 169\n",
      "saved weights\n",
      "20 Average Episode Reward: 3.97177324377202 Replay buffer size: 337\n",
      "saved weights\n",
      "30 Average Episode Reward: 3.854186111451173 Replay buffer size: 501\n",
      "40 Average Episode Reward: 2.753283592436644 Replay buffer size: 644\n",
      "50 Average Episode Reward: 4.314710583100586 Replay buffer size: 824\n",
      "saved weights\n",
      "60 Average Episode Reward: 4.127975911177465 Replay buffer size: 1010\n",
      "70 Average Episode Reward: 3.793837606575761 Replay buffer size: 1180\n",
      "80 Average Episode Reward: 3.109844914225509 Replay buffer size: 1327\n",
      "90 Average Episode Reward: 3.5487815816408066 Replay buffer size: 1487\n",
      "100 Average Episode Reward: 4.504338764005302 Replay buffer size: 1676\n",
      "saved weights\n",
      "110 Average Episode Reward: 4.3227018349822215 Replay buffer size: 1861\n",
      "120 Average Episode Reward: 4.062005235808198 Replay buffer size: 2034\n",
      "130 Average Episode Reward: 5.184022444937628 Replay buffer size: 2233\n",
      "saved weights\n",
      "140 Average Episode Reward: 4.285561947255436 Replay buffer size: 2414\n",
      "150 Average Episode Reward: 3.2536003402203653 Replay buffer size: 2566\n",
      "160 Average Episode Reward: 5.171127705141832 Replay buffer size: 2765\n",
      "170 Average Episode Reward: 4.2233390468012875 Replay buffer size: 2935\n",
      "180 Average Episode Reward: 4.4982679972318165 Replay buffer size: 3131\n",
      "190 Average Episode Reward: 5.058214215541527 Replay buffer size: 3336\n",
      "200 Average Episode Reward: 4.352435601235557 Replay buffer size: 3515\n",
      "210 Average Episode Reward: 4.744165508161809 Replay buffer size: 3706\n",
      "220 Average Episode Reward: 4.352101170166678 Replay buffer size: 3890\n",
      "230 Average Episode Reward: 3.5526999443364944 Replay buffer size: 4052\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m action \u001b[38;5;241m=\u001b[39m greedy\u001b[38;5;241m.\u001b[39mget_action(agent, state)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# print(action)\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done: \n\u001b[0;32m     32\u001b[0m     reward \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Git\\ai-in-robotics-41118\\fsae\\envs.py:92\u001b[0m, in \u001b[0;36mRandomTrackEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetector\u001b[38;5;241m.\u001b[39mreproject_object_to_3d(boxes))\n\u001b[0;32m     90\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeStep)\n\u001b[1;32m---> 92\u001b[0m carpos, carorn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_p\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetBasePositionAndOrientation\u001b[49m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcar\u001b[38;5;241m.\u001b[39mcar)\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# goalpos, goalorn = self._p.getBasePositionAndOrientation(\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m#     self.goal_object.goal\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[0;32m     96\u001b[0m car_ob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgetExtendedObservation()\n",
      "File \u001b[1;32mc:\\Users\\conno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pybullet_utils\\bullet_client.py:46\u001b[0m, in \u001b[0;36mBulletClient.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m pybullet\u001b[38;5;241m.\u001b[39merror:\n\u001b[0;32m     44\u001b[0m       \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name):\n\u001b[0;32m     47\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Inject the client id into Bullet functions.\"\"\"\u001b[39;00m\n\u001b[0;32m     48\u001b[0m   attribute \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(pybullet, name)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize the agent, replay buffer, and environment\n",
    "state_dim = 8 # Dimension of the state space\n",
    "action_dim = 2 # Dimension of the action space\n",
    "hidden_dim = 256\n",
    "max_action = (1,0.6) # Maximum value of the action\n",
    "num_episodes = 1000\n",
    "max_steps = 25\n",
    "batch_size = 500\n",
    "\n",
    "replay_buffer = ReplayBuffer(buffer_size=50000, state_dim=state_dim, action_dim=action_dim)\n",
    "# replay_buffer.load_from_csv(\"replay_buffers/replayBuffer_teleop_test.csv\")\n",
    "agent = DDPGAgent(state_dim, action_dim, hidden_dim, replay_buffer, max_action)\n",
    "greedy = EpsilonGreedy(1, 0.01, 0.001, num_episodes, 0.4)\n",
    "if replay_buffer.size > batch_size:\n",
    "    print(\"Replay buffer size after load: \", replay_buffer.size, \" vs Batch Size: \", batch_size)\n",
    "    agent.train(batch_size)\n",
    "\n",
    "rewards = []\n",
    "avg_rewards = []\n",
    "best_reward  = 0\n",
    "\n",
    "# Training loop\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset(seed=episode)\n",
    "    episode_reward = 0\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        action = greedy.get_action(agent, state)\n",
    "        # print(action)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        if done: \n",
    "            reward -= 1\n",
    "        \n",
    "        replay_buffer.add(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "        if replay_buffer.size > batch_size:\n",
    "            agent.train(batch_size)\n",
    "            # print(\"Training\")\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    rewards.append(episode_reward)\n",
    "\n",
    "    if (episode % 10 == 0):\n",
    "        avg_reward = np.mean(np.asarray(rewards))\n",
    "        print(f\"{episode} Average Episode Reward: {avg_reward} Replay buffer size: {replay_buffer.size}\")\n",
    "        rewards.clear()\n",
    "        avg_rewards.append(avg_reward)\n",
    "        if avg_reward > best_reward:\n",
    "            print(\"saved weights\")\n",
    "            agent.save_weights()\n",
    "            best_reward = avg_reward\n",
    "\n",
    "    greedy.incr_step()\n",
    "\n",
    "agent.save_weights(best=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save_weights(best=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer.save_as_csv(\"replayBuffer_train_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    env.close()\n",
    "except:\n",
    "    pass    \n",
    "env = RandomTrackEnv(render_mode='tp_camera', seed=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[          1    -0.24484]\n",
      "[          1    -0.23563]\n",
      "[          1    -0.17694]\n",
      "[          1   -0.057106]\n",
      "[          1    0.041586]\n",
      "[          1    0.086771]\n",
      "[          1     0.10193]\n",
      "[          1    0.054719]\n",
      "[          1  -0.0035542]\n",
      "[          1   -0.022078]\n",
      "[          1    -0.39872]\n",
      "[          1     0.19029]\n",
      "[          1     0.14764]\n",
      "[          1   -0.042296]\n",
      "[          1   -0.012219]\n",
      "[          1    0.035005]\n",
      "[          1     -0.4182]\n",
      "[          1     0.26932]\n",
      "[          1   -0.019044]\n",
      "[          1      0.1066]\n",
      "[          1    -0.44693]\n",
      "[          1     0.26325]\n",
      "[          1    0.087065]\n",
      "[          1      -0.264]\n",
      "[          1   -0.056108]\n",
      "[          1    0.047416]\n",
      "[          1     -0.1079]\n",
      "[          1     0.03135]\n",
      "[          1     0.10418]\n",
      "[          1    -0.13084]\n",
      "[          1    0.028727]\n",
      "[          1    -0.29646]\n",
      "[          1   0.0088188]\n",
      "[          1     0.24401]\n",
      "[          1   -0.034907]\n",
      "[          1   -0.040003]\n",
      "[          1    0.081847]\n",
      "[          1    0.098868]\n",
      "[          1    0.020599]\n",
      "[          1   -0.053186]\n",
      "[          1    -0.49428]\n",
      "[          1     0.27538]\n",
      "[          1    0.083656]\n",
      "[          1    0.073601]\n",
      "[          1    0.094815]\n",
      "[          1   -0.046285]\n",
      "[          1     0.15737]\n",
      "[          1   -0.039638]\n",
      "[          1    -0.30397]\n",
      "[          1     0.16061]\n",
      "[          1    0.096441]\n",
      "[          1     0.15464]\n",
      "[          1   -0.059103]\n",
      "[          1    0.044458]\n",
      "[          1   -0.059056]\n",
      "[          1    0.066144]\n",
      "[          1     0.10476]\n",
      "[          1     0.01497]\n",
      "[          1   0.0032691]\n",
      "[          1      -0.478]\n",
      "[          1    -0.54553]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m      6\u001b[0m     action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mget_action(state)\n\u001b[1;32m----> 7\u001b[0m     next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(action)\n",
      "File \u001b[1;32mc:\\Git\\ai-in-robotics-41118\\fsae\\envs.py:90\u001b[0m, in \u001b[0;36mRandomTrackEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     88\u001b[0m         boxes \u001b[38;5;241m=\u001b[39m results\u001b[38;5;241m.\u001b[39mpred[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m, :\u001b[38;5;241m4\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[0;32m     89\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetector\u001b[38;5;241m.\u001b[39mreproject_object_to_3d(boxes))\n\u001b[1;32m---> 90\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeStep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m carpos, carorn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_p\u001b[38;5;241m.\u001b[39mgetBasePositionAndOrientation(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcar\u001b[38;5;241m.\u001b[39mcar)\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# goalpos, goalorn = self._p.getBasePositionAndOrientation(\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m#     self.goal_object.goal\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "done = False\n",
    "state = env.reset(seed=4)\n",
    "agent.load_weights()\n",
    "\n",
    "while not done:\n",
    "    action = agent.get_action(state)\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    state = next_state\n",
    "    print(action)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_quiz_3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
