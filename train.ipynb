{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: Apr 10 2024 13:56:47\n"
     ]
    }
   ],
   "source": [
    "from ddpg import ReplayBuffer, DDPGAgent\n",
    "from fsae.envs import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "argv[0]=\n",
      "argv[0]=\n"
     ]
    }
   ],
   "source": [
    "env = RandomTrackEnv(render_mode=None, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replay buffer size after load:  5537  vs Batch Size:  128\n",
      "Average Episode Reward: -1.693944340102678\n",
      "Average Episode Reward: 0.644396338835515\n",
      "Average Episode Reward: 7.853827148097823\n",
      "Average Episode Reward: 9.392630518819573\n",
      "Average Episode Reward: 8.823770636627893\n",
      "Average Episode Reward: 8.640582509763986\n",
      "Average Episode Reward: 4.137006891953139\n",
      "Average Episode Reward: 9.909630295338756\n",
      "Average Episode Reward: 6.688111533792127\n",
      "Average Episode Reward: 10.025118343443292\n",
      "Average Episode Reward: 9.457149733703837\n",
      "Average Episode Reward: 8.554348353643281\n",
      "Average Episode Reward: 13.018723908725374\n",
      "Average Episode Reward: 13.198163002957324\n",
      "Average Episode Reward: 9.6859246166967\n",
      "Average Episode Reward: 5.3544860610885365\n",
      "Average Episode Reward: 6.284100620848362\n",
      "Average Episode Reward: 9.238972887098972\n",
      "Average Episode Reward: 11.399988115024803\n",
      "Average Episode Reward: 8.228044584331652\n",
      "Average Episode Reward: 4.83950382163657\n",
      "Average Episode Reward: 5.138843662505842\n",
      "Average Episode Reward: 5.952206657401773\n",
      "Average Episode Reward: 4.7552212065648\n",
      "Average Episode Reward: 4.335935735753979\n",
      "Average Episode Reward: 7.810839503294187\n",
      "Average Episode Reward: 4.943891348636922\n",
      "Average Episode Reward: 5.8473248508508595\n",
      "Average Episode Reward: 7.9666631485659325\n",
      "Average Episode Reward: 5.7818457539262464\n",
      "Average Episode Reward: 5.391217959334897\n",
      "Average Episode Reward: 6.43266856069467\n",
      "Average Episode Reward: 5.992519225413913\n",
      "Average Episode Reward: 3.5389917909693027\n",
      "Average Episode Reward: 7.44329212898622\n",
      "Average Episode Reward: 4.3118256752260455\n",
      "Average Episode Reward: 5.89669908768186\n",
      "Average Episode Reward: 5.810352248904785\n",
      "Average Episode Reward: 3.5973736677326342\n",
      "Average Episode Reward: 7.5176042601489765\n",
      "Average Episode Reward: 4.747477128324158\n",
      "Average Episode Reward: 3.748359088835256\n",
      "Average Episode Reward: 3.925257029061086\n",
      "Average Episode Reward: 6.3983166211889015\n",
      "Average Episode Reward: 3.8747206047972753\n",
      "Average Episode Reward: 3.6444834853224832\n",
      "Average Episode Reward: 4.811616442285867\n",
      "Average Episode Reward: 5.273049734659167\n",
      "Average Episode Reward: 3.959409928969854\n",
      "Average Episode Reward: 3.535518303856204\n",
      "Average Episode Reward: 6.240330886559624\n",
      "Average Episode Reward: 5.751408603953519\n",
      "Average Episode Reward: 8.197794504880923\n",
      "Average Episode Reward: 5.291767271474993\n",
      "Average Episode Reward: 5.708262811386691\n",
      "Average Episode Reward: 6.607457976002864\n",
      "Average Episode Reward: 5.434101306467111\n",
      "Average Episode Reward: 8.058280837174772\n",
      "Average Episode Reward: 7.724147520371505\n",
      "Average Episode Reward: 7.089445054441951\n",
      "Average Episode Reward: 5.565553837749927\n",
      "Average Episode Reward: 6.45310815898286\n",
      "Average Episode Reward: 9.982394967339225\n",
      "Average Episode Reward: 7.237999341286601\n",
      "Average Episode Reward: 7.211871348693839\n",
      "Average Episode Reward: 8.246979447508185\n",
      "Average Episode Reward: 11.444130981866865\n",
      "Average Episode Reward: 8.042721496175268\n",
      "Average Episode Reward: 6.087288264892751\n",
      "Average Episode Reward: 6.269934137400175\n",
      "Average Episode Reward: 9.83983380132012\n",
      "Average Episode Reward: 9.106907205563743\n",
      "Average Episode Reward: 10.302162259157516\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_steps):\n\u001b[1;32m     28\u001b[0m     action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mget_action(state)\n\u001b[0;32m---> 29\u001b[0m     next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m done: \n\u001b[1;32m     31\u001b[0m         reward \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n",
      "File \u001b[0;32m~/ai/ai-in-robotics-41118/fsae/envs.py:69\u001b[0m, in \u001b[0;36mRandomTrackEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcar\u001b[38;5;241m.\u001b[39mapply_action(action)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_actionRepeat):\n\u001b[0;32m---> 69\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_p\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstepSimulation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_renders:\n\u001b[1;32m     71\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeStep)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize the agent, replay buffer, and environment\n",
    "state_dim = 12 # Dimension of the state space\n",
    "action_dim = 2 # Dimension of the action space\n",
    "hidden_dim = 256\n",
    "max_action = (1,0.6) # Maximum value of the action\n",
    "num_episodes = 1000\n",
    "max_steps = 500\n",
    "batch_size = 128\n",
    "\n",
    "replay_buffer = ReplayBuffer(buffer_size=50000, state_dim=state_dim, action_dim=action_dim)\n",
    "replay_buffer.load_from_csv(\"replay_buffers/replayBuffer_teleop_test.csv\")\n",
    "agent = DDPGAgent(state_dim, action_dim, hidden_dim, replay_buffer, max_action)\n",
    "\n",
    "if replay_buffer.size > batch_size:\n",
    "    print(\"Replay buffer size after load: \", replay_buffer.size, \" vs Batch Size: \", batch_size)\n",
    "    agent.train(batch_size)\n",
    "\n",
    "rewards = []\n",
    "avg_rewards = []\n",
    "best_reward  = 0\n",
    "\n",
    "# Training loop\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset(seed=episode)\n",
    "    episode_reward = 0\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        if done: \n",
    "            reward -= 0.5\n",
    "        \n",
    "        replay_buffer.add(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "        if replay_buffer.size > batch_size:\n",
    "            agent.train(batch_size)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    rewards.append(episode_reward)\n",
    "\n",
    "    if (episode % 10 == 0):\n",
    "        avg_reward = np.mean(np.asarray(rewards))\n",
    "        print(f\"Average Episode Reward: {avg_reward}\")\n",
    "        rewards.clear()\n",
    "        avg_rewards.append(avg_reward)\n",
    "        if avg_reward > best_reward:\n",
    "            agent.save_weights()\n",
    "            best_reward = avg_reward\n",
    "\n",
    "agent.save_weights(best=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save_weights(best=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer.save_as_csv(\"replayBuffer_train_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "argv[0]=\n",
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "argc=3\n",
      "argv[0] = --unused\n",
      "argv[1] = \n",
      "argv[2] = --start_demo_name=Physics Server\n",
      "ExampleBrowserThreadFunc started\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "Creating context\n",
      "Created GL 3.3 context\n",
      "Direct GLX rendering context obtained\n",
      "Making context current\n",
      "GL_VENDOR=NVIDIA Corporation\n",
      "GL_RENDERER=NVIDIA GeForce RTX 3090/PCIe/SSE2\n",
      "GL_VERSION=3.3.0 NVIDIA 535.171.04\n",
      "GL_SHADING_LANGUAGE_VERSION=3.30 NVIDIA via Cg compiler\n",
      "pthread_getconcurrency()=0\n",
      "Version = 3.3.0 NVIDIA 535.171.04\n",
      "Vendor = NVIDIA Corporation\n",
      "Renderer = NVIDIA GeForce RTX 3090/PCIe/SSE2\n",
      "b3Printf: Selected demo: Physics Server\n",
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "MotionThreadFunc thread started\n",
      "ven = NVIDIA Corporation\n",
      "ven = NVIDIA Corporation\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    env.close()\n",
    "except:\n",
    "    pass    \n",
    "env = RandomTrackEnv(render_mode='tp_camera', seed=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2637833876602505\n",
      "0.16083727998001168\n",
      "0.17515989475168403\n",
      "0.1691778336034403\n",
      "0.16127845114205974\n",
      "0.15196942530656532\n",
      "0.13880068842563076\n",
      "0.11811161348623767\n",
      "0.09009490381997454\n",
      "0.054289855931912934\n",
      "0.008192499587276414\n",
      "-0.03860371049113631\n",
      "-0.08736608672028434\n",
      "-0.14738489976207503\n",
      "-0.22223007957019114\n",
      "-0.31327372868716474\n",
      "-0.02142284210754586\n",
      "0.05046516297867942\n",
      "-0.015462809580798043\n",
      "6.325198821177658e-05\n",
      "-0.0018939770167392123\n",
      "0.008396187037121194\n",
      "0.004475055606903422\n",
      "0.004292370185067895\n",
      "-0.0008981559455429888\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m      6\u001b[0m     action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mget_action(state)\n\u001b[0;32m----> 7\u001b[0m     next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(reward)\n",
      "File \u001b[0;32m~/ai/ai-in-robotics-41118/fsae/envs.py:71\u001b[0m, in \u001b[0;36mRandomTrackEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_p\u001b[38;5;241m.\u001b[39mstepSimulation()\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_renders:\n\u001b[0;32m---> 71\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeStep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m carpos, carorn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_p\u001b[38;5;241m.\u001b[39mgetBasePositionAndOrientation(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcar\u001b[38;5;241m.\u001b[39mcar)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# goalpos, goalorn = self._p.getBasePositionAndOrientation(\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m#     self.goal_object.goal\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "done = False\n",
    "state = env.reset(seed=4)\n",
    "agent.load_weights()\n",
    "\n",
    "while not done:\n",
    "    action = agent.get_action(state)\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    print(reward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_quiz_3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
