{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddpg import ReplayBuffer, DDPGAgent, EpsilonGreedy\n",
    "from fsae.envs import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    env.close()\n",
    "except:\n",
    "    pass \n",
    "env = RandomTrackEnv(render_mode=None, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = np.random.normal(0,0.5, 1000)\n",
    "plt.hist(samples, bins=20, edgecolor='black')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replay buffer size after load:  48727  vs Batch Size:  500\n",
      "10: Average Episode Reward: 4.248586702131805 Replay buffer size: 48924\n",
      "saved weights\n",
      "20: Average Episode Reward: 4.294611323310932 Replay buffer size: 49102\n",
      "saved weights\n",
      "30: Average Episode Reward: 5.236380642183938 Replay buffer size: 49315\n",
      "saved weights\n",
      "40: Average Episode Reward: 4.833428367113659 Replay buffer size: 49510\n",
      "50: Average Episode Reward: 5.011837664990888 Replay buffer size: 49715\n",
      "60: Average Episode Reward: 7.013601499865165 Replay buffer size: 49983\n",
      "saved weights\n",
      "70: Average Episode Reward: 5.903436201327796 Replay buffer size: 50000\n",
      "80: Average Episode Reward: 9.94751588599089 Replay buffer size: 50000\n",
      "saved weights\n",
      "90: Average Episode Reward: 9.783763481364918 Replay buffer size: 50000\n",
      "100: Average Episode Reward: 10.204718122635938 Replay buffer size: 50000\n",
      "saved weights\n",
      "110: Average Episode Reward: 9.950564131605894 Replay buffer size: 50000\n",
      "120: Average Episode Reward: 7.072330378094703 Replay buffer size: 50000\n",
      "130: Average Episode Reward: 11.43305712107447 Replay buffer size: 50000\n",
      "saved weights\n",
      "140: Average Episode Reward: 15.72788345095051 Replay buffer size: 50000\n",
      "saved weights\n",
      "150: Average Episode Reward: 12.442885694811107 Replay buffer size: 50000\n",
      "160: Average Episode Reward: 27.6656539721302 Replay buffer size: 50000\n",
      "saved weights\n",
      "170: Average Episode Reward: 16.149293038608086 Replay buffer size: 50000\n",
      "180: Average Episode Reward: 16.356155731412457 Replay buffer size: 50000\n",
      "190: Average Episode Reward: 17.873194950234137 Replay buffer size: 50000\n",
      "200: Average Episode Reward: 15.093970963408045 Replay buffer size: 50000\n",
      "210: Average Episode Reward: 13.245167072710222 Replay buffer size: 50000\n",
      "220: Average Episode Reward: 22.314159945725457 Replay buffer size: 50000\n",
      "230: Average Episode Reward: 27.00577327946825 Replay buffer size: 50000\n",
      "240: Average Episode Reward: 20.51094024465799 Replay buffer size: 50000\n",
      "250: Average Episode Reward: 18.555797653353014 Replay buffer size: 50000\n",
      "260: Average Episode Reward: 19.778448386709034 Replay buffer size: 50000\n",
      "270: Average Episode Reward: 24.073800568647783 Replay buffer size: 50000\n",
      "280: Average Episode Reward: 27.79938214220774 Replay buffer size: 50000\n",
      "saved weights\n",
      "290: Average Episode Reward: 33.02660632428707 Replay buffer size: 50000\n",
      "saved weights\n",
      "300: Average Episode Reward: 33.789044663939855 Replay buffer size: 50000\n",
      "saved weights\n",
      "310: Average Episode Reward: 38.57627968925125 Replay buffer size: 50000\n",
      "saved weights\n",
      "320: Average Episode Reward: 38.98503600457642 Replay buffer size: 50000\n",
      "saved weights\n",
      "330: Average Episode Reward: 30.04600387836539 Replay buffer size: 50000\n",
      "340: Average Episode Reward: 41.247844101645384 Replay buffer size: 50000\n",
      "saved weights\n",
      "350: Average Episode Reward: 51.949134078858876 Replay buffer size: 50000\n",
      "saved weights\n",
      "360: Average Episode Reward: 35.30549097351894 Replay buffer size: 50000\n",
      "370: Average Episode Reward: 43.987641978276756 Replay buffer size: 50000\n",
      "380: Average Episode Reward: 51.57428523541718 Replay buffer size: 50000\n",
      "390: Average Episode Reward: 56.39436087450034 Replay buffer size: 50000\n",
      "saved weights\n",
      "400: Average Episode Reward: 85.14476118471134 Replay buffer size: 50000\n",
      "saved weights\n",
      "410: Average Episode Reward: 66.34750825715248 Replay buffer size: 50000\n",
      "420: Average Episode Reward: 47.81758976025593 Replay buffer size: 50000\n",
      "430: Average Episode Reward: 51.0350285046402 Replay buffer size: 50000\n",
      "440: Average Episode Reward: 55.82826387191814 Replay buffer size: 50000\n",
      "450: Average Episode Reward: 56.29881853482006 Replay buffer size: 50000\n",
      "460: Average Episode Reward: 102.80538564842827 Replay buffer size: 50000\n",
      "saved weights\n",
      "470: Average Episode Reward: 54.92532144181014 Replay buffer size: 50000\n",
      "480: Average Episode Reward: 68.25746505860316 Replay buffer size: 50000\n",
      "490: Average Episode Reward: 68.88424547516877 Replay buffer size: 50000\n",
      "500: Average Episode Reward: 82.96329702424794 Replay buffer size: 50000\n",
      "510: Average Episode Reward: 87.55105282070083 Replay buffer size: 50000\n",
      "520: Average Episode Reward: 71.05209628132454 Replay buffer size: 50000\n",
      "530: Average Episode Reward: 42.836703382400046 Replay buffer size: 50000\n",
      "540: Average Episode Reward: 30.015845742115694 Replay buffer size: 50000\n",
      "550: Average Episode Reward: 37.67870363026167 Replay buffer size: 50000\n",
      "560: Average Episode Reward: 48.0651524974061 Replay buffer size: 50000\n",
      "570: Average Episode Reward: 49.02573847862198 Replay buffer size: 50000\n",
      "580: Average Episode Reward: 51.2493529417291 Replay buffer size: 50000\n",
      "590: Average Episode Reward: 63.0608029425204 Replay buffer size: 50000\n",
      "600: Average Episode Reward: 74.54184217300623 Replay buffer size: 50000\n",
      "610: Average Episode Reward: 42.569706757682944 Replay buffer size: 50000\n",
      "620: Average Episode Reward: 42.638989825257205 Replay buffer size: 50000\n",
      "630: Average Episode Reward: 29.64285978314018 Replay buffer size: 50000\n",
      "640: Average Episode Reward: 59.850134472768715 Replay buffer size: 50000\n",
      "650: Average Episode Reward: 32.59113222766641 Replay buffer size: 50000\n",
      "660: Average Episode Reward: 34.926169504151915 Replay buffer size: 50000\n",
      "670: Average Episode Reward: 21.460556890803584 Replay buffer size: 50000\n",
      "680: Average Episode Reward: 24.51645176221465 Replay buffer size: 50000\n",
      "690: Average Episode Reward: 31.52486722989527 Replay buffer size: 50000\n",
      "700: Average Episode Reward: 22.710765817976448 Replay buffer size: 50000\n",
      "710: Average Episode Reward: 33.950196992897396 Replay buffer size: 50000\n",
      "720: Average Episode Reward: 31.877786083055998 Replay buffer size: 50000\n",
      "730: Average Episode Reward: 72.18549454190953 Replay buffer size: 50000\n",
      "740: Average Episode Reward: 52.45043160661678 Replay buffer size: 50000\n",
      "750: Average Episode Reward: 45.37728461081075 Replay buffer size: 50000\n",
      "760: Average Episode Reward: 56.660779973263075 Replay buffer size: 50000\n",
      "770: Average Episode Reward: 26.693646087585257 Replay buffer size: 50000\n",
      "780: Average Episode Reward: 34.889097310568175 Replay buffer size: 50000\n",
      "790: Average Episode Reward: 42.68422959697309 Replay buffer size: 50000\n",
      "800: Average Episode Reward: 51.42724412763514 Replay buffer size: 50000\n",
      "810: Average Episode Reward: 73.12227220175495 Replay buffer size: 50000\n",
      "820: Average Episode Reward: 56.39732360741883 Replay buffer size: 50000\n",
      "830: Average Episode Reward: 47.95788851412065 Replay buffer size: 50000\n",
      "840: Average Episode Reward: 70.58888771094003 Replay buffer size: 50000\n",
      "850: Average Episode Reward: 46.442646943853404 Replay buffer size: 50000\n",
      "860: Average Episode Reward: 39.853222461478964 Replay buffer size: 50000\n",
      "870: Average Episode Reward: 50.121527261752846 Replay buffer size: 50000\n",
      "880: Average Episode Reward: 21.80089818048218 Replay buffer size: 50000\n",
      "890: Average Episode Reward: 37.17640389088239 Replay buffer size: 50000\n",
      "900: Average Episode Reward: 25.446925437260425 Replay buffer size: 50000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 34\u001b[0m\n\u001b[0;32m     32\u001b[0m action \u001b[38;5;241m=\u001b[39m greedy\u001b[38;5;241m.\u001b[39mget_action(agent, state)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# print(action)\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done: \n\u001b[0;32m     36\u001b[0m     reward \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Git\\ai-in-robotics-41118\\fsae\\envs.py:175\u001b[0m, in \u001b[0;36mRandomTrackEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprev_dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprojectAndFindDistance(\n\u001b[0;32m    168\u001b[0m         \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcentres)[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    169\u001b[0m         \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcentres)[\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m    170\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcar\u001b[38;5;241m.\u001b[39mget_observation()[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m2\u001b[39m],\n\u001b[0;32m    171\u001b[0m     )\n\u001b[0;32m    173\u001b[0m ob \u001b[38;5;241m=\u001b[39m car_ob\n\u001b[0;32m    174\u001b[0m visual_cones \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(\n\u001b[1;32m--> 175\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetConesTransformedAndSorted\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32\n\u001b[0;32m    176\u001b[0m )\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m visual_cones, reward, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone, \u001b[38;5;28mdict\u001b[39m()\n",
      "File \u001b[1;32mc:\\Git\\ai-in-robotics-41118\\fsae\\envs.py:492\u001b[0m, in \u001b[0;36mRandomTrackEnv.getConesTransformedAndSorted\u001b[1;34m(self, num_cones, detected_cones)\u001b[0m\n\u001b[0;32m    489\u001b[0m r_cones \u001b[38;5;241m=\u001b[39m [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcones \u001b[38;5;28;01mif\u001b[39;00m c\u001b[38;5;241m.\u001b[39mcolor \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myellow\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    491\u001b[0m \u001b[38;5;66;03m# Convert all cones to car reference frame\u001b[39;00m\n\u001b[1;32m--> 492\u001b[0m l_cones \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreframeToCar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcone\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m], c\u001b[38;5;241m.\u001b[39mcolor) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m l_cones]\n\u001b[0;32m    493\u001b[0m r_cones \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreframeToCar(c\u001b[38;5;241m.\u001b[39mcone)[:\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m], c\u001b[38;5;241m.\u001b[39mcolor) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m r_cones]\n\u001b[0;32m    495\u001b[0m \u001b[38;5;66;03m# Filter out cones outside of 90deg FOV\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Git\\ai-in-robotics-41118\\fsae\\envs.py:556\u001b[0m, in \u001b[0;36mRandomTrackEnv.reframeToCar\u001b[1;34m(self, target_object)\u001b[0m\n\u001b[0;32m    553\u001b[0m target_world_matrix_inv \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39minv(target_world_matrix)\n\u001b[0;32m    555\u001b[0m \u001b[38;5;66;03m# Compute the transformation from world object to the target object frame\u001b[39;00m\n\u001b[1;32m--> 556\u001b[0m transformation_to_target_frame \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_world_matrix_inv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworld_matrix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m transformation_to_target_frame\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize the agent, replay buffer, and environment\n",
    "state_dim = 8 # Dimension of the state space\n",
    "action_dim = 2 # Dimension of the action space\n",
    "hidden_dim = 256\n",
    "max_action = (1,0.6) # Maximum value of the action\n",
    "num_episodes = 1000\n",
    "max_steps = 1000\n",
    "batch_size = 500\n",
    "\n",
    "replay_buffer = ReplayBuffer(buffer_size=50000, state_dim=state_dim, action_dim=action_dim)\n",
    "replay_buffer.load_from_csv(\"replayBuffer_train_test_50000.csv\")\n",
    "agent = DDPGAgent(state_dim, action_dim, hidden_dim, replay_buffer, max_action)\n",
    "agent.load_weights()\n",
    "\n",
    "#greedy = EpsilonGreedy(1, 0.3, 0.01, num_episodes, 0.4)\n",
    "greedy = EpsilonGreedy(1, 0.01, 0.01, num_episodes, 0.4)\n",
    "if replay_buffer.size > batch_size:\n",
    "    print(\"Replay buffer size after load: \", replay_buffer.size, \" vs Batch Size: \", batch_size)\n",
    "    agent.train(batch_size)\n",
    "\n",
    "rewards = []\n",
    "avg_rewards = []\n",
    "best_reward  = 0\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset(seed=episode)\n",
    "    episode_reward = 0\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        action = greedy.get_action(agent, state)\n",
    "        # print(action)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        if done: \n",
    "            reward -= 1\n",
    "        \n",
    "        replay_buffer.add(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "        if replay_buffer.size > batch_size:\n",
    "            agent.train(batch_size)\n",
    "            # print(\"Training\")\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    rewards.append(episode_reward)\n",
    "\n",
    "    if (episode % 10 == 0):\n",
    "        if episode == 0: \n",
    "            continue\n",
    "        avg_reward = np.mean(np.asarray(rewards))\n",
    "        print(f\"{episode}: Average Episode Reward: {avg_reward} Replay buffer size: {replay_buffer.size}\")\n",
    "        rewards.clear()\n",
    "        avg_rewards.append(avg_reward)\n",
    "        if avg_reward > best_reward:\n",
    "            print(\"saved weights\")\n",
    "            agent.save_weights()\n",
    "            best_reward = avg_reward \n",
    "    greedy.incr_step()\n",
    "\n",
    "agent.save_weights(best=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Plotting rewards\n",
    "plt.plot(avg_reward)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.title('Average Reward per 10 Episodes')\n",
    "plt.show()\n",
    "\n",
    "# Save rewards to CSV\n",
    "with open('average_rewards.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['Episode', 'Average Reward'])\n",
    "    for episode, reward in enumerate(avg_rewards):\n",
    "        writer.writerow([episode, reward])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save_weights(best=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer.save_as_csv(\"replayBuffer_train_test_1000_eposh.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    env.close()\n",
    "except:\n",
    "    pass    \n",
    "env = RandomTrackEnv(render_mode='tp_camera', seed=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the agent, replay buffer, and environment\n",
    "state_dim = 8 # Dimension of the state space\n",
    "action_dim = 2 # Dimension of the action space\n",
    "hidden_dim = 256\n",
    "max_action = (1,0.6) # Maximum value of the action\n",
    "num_episodes = 1000\n",
    "max_steps = 25\n",
    "batch_size = 500\n",
    "\n",
    "replay_buffer = ReplayBuffer(buffer_size=50000, state_dim=state_dim, action_dim=action_dim)\n",
    "replay_buffer.load_from_csv(\"replayBuffer_train_test_1000_eposh.csv\")\n",
    "agent = DDPGAgent(state_dim, action_dim, hidden_dim, replay_buffer, max_action)\n",
    "agent.load_weights()\n",
    "agent.train(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 : total reward: 17.888857977443415\n",
      "100 : total reward: 35.66990133411456\n",
      "150 : total reward: 53.372084246898154\n",
      "200 : total reward: 71.89597171081152\n",
      "244 : Finished simulation. Total reward: 84.91537914557964\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "state = env.reset(seed=4)\n",
    "agent.load_weights()\n",
    "\n",
    "total_reward = 0\n",
    "counter = 0\n",
    "\n",
    "while not done:\n",
    "    action = agent.get_action(state)\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    state = next_state\n",
    "\n",
    "    total_reward += reward\n",
    "    counter += 1\n",
    "\n",
    "    if counter % 50 == 0:\n",
    "        print(f'{counter} : total reward: {total_reward}')\n",
    "    #print(action)\n",
    "\n",
    "print(f'{counter} : Finished simulation. Total reward: {total_reward}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_quiz_3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
